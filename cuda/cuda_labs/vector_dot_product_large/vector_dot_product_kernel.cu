#ifndef _VECTOR_DOT_PRODUCT_KERNEL_H_
#define _VECTOR_DOT_PRODUCT_KERNEL_H_

#define THREAD_BLOCK_SIZE 256
#define NUM_BLOCKS 240 // Define the size of a tile

__global__ void vector_dot_product_kernel(float *A, float *B, float *C, unsigned int num_elements)
{
	__shared__ float sum_per_thread[THREAD_BLOCK_SIZE];	
	unsigned int thread_id = blockIdx.x * blockDim.x + threadIdx.x; // Obtain the index of the thread
	unsigned int stride = blockDim.x * gridDim.x; 
	float sum = 0.0f; 
	unsigned int i = thread_id; 

	while(i < num_elements){
			  sum += A[i] * B[i];
			  i += stride;
	}

	sum_per_thread[threadIdx.x] = sum; // Copy sum to shared memory
	__syncthreads();

	// Reduce the values generated by the thread block to a single value to be sent back to the CPU
	// The following code assumes that the number of threads per block is power of two
	i = blockDim.x/2;
	while(i != 0){
			  if(threadIdx.x < i) 
						 sum_per_thread[threadIdx.x] += sum_per_thread[threadIdx.x + i];
			  __syncthreads();
			  i /= 2;
	}

	// Write the partial sum computed by this thread block to global memory
	if(threadIdx.x == 0)
			  C[blockIdx.x] = sum_per_thread[0];
}


/* This function uses a compare and swap technique to acquire a mutex/lock. */
__device__ void lock(int *mutex){
		  while(atomicCAS(mutex, 0, 1) != 0);
}

/* This function uses an atomic exchange operation to release the mutex/lock. */
__device__ void unlock(int *mutex)
{
		  atomicExch(mutex, 0);
}


__global__ void vector_dot_product_kernel_v2_sp(float *A, float *B, float *C, unsigned int num_elements, int *mutex)
{
	__shared__ float sum_per_thread[THREAD_BLOCK_SIZE];
	unsigned int thread_id = blockIdx.x * blockDim.x + threadIdx.x; // Obtain the index of the thread
	unsigned int stride = blockDim.x * gridDim.x; 
	float sum = 0.0f; 
	unsigned int i = thread_id; 

	while(i < num_elements){
			  sum += A[i] * B[i];
			  i += stride;
	}

	sum_per_thread[threadIdx.x] = sum; // Copy sum to shared memory
	__syncthreads();

	// Reduce the values generated by the thread block to a single value to be sent back to the CPU
	// The following code assumes that the number of threads per block is power of two
	i = blockDim.x/2;
	while(i != 0){
			  if(threadIdx.x < i) 
						 sum_per_thread[threadIdx.x] += sum_per_thread[threadIdx.x + i];
			  __syncthreads();
			  i /= 2;
	}

	// Write the partial sum computed by this thread block to global memory
	if(threadIdx.x == 0){
			  lock(mutex);
			  C[0] += sum_per_thread[0];
			  unlock(mutex);
	}
}

__global__ void vector_dot_product_kernel_v2_kahan(float *A, float *B, float *C, unsigned int num_elements, int *mutex)
{
	__shared__ float sum_per_thread[THREAD_BLOCK_SIZE];
	unsigned int thread_id = blockIdx.x * blockDim.x + threadIdx.x; // Obtain the index of the thread
	unsigned int stride = blockDim.x * gridDim.x; 
	float sum = 0.0f; 
	float recovered_bits = 0.0f;
	float temp, y;

	unsigned int i = thread_id; 
	while(i < num_elements){
			  y = (A[i]*B[i]) - recovered_bits;
			  temp = sum + y; // May lose the lower-order bits here
			  recovered_bits = (temp - sum) - y; // Recover lower-order bits; we will add this value to the value of y in the next iteration
			  sum = temp; 
			  i += stride;
	}

	sum_per_thread[threadIdx.x] = sum; // Copy sum to shared memory
	__syncthreads();

	// Reduce the values generated by the thread block to a single value to be sent back to the CPU
	// The following code assumes that the number of threads per block is power of two
	i = blockDim.x/2;
	while(i != 0){
			  if(threadIdx.x < i) 
						 sum_per_thread[threadIdx.x] += sum_per_thread[threadIdx.x + i];
			  __syncthreads();
			  i /= 2;
	}

	// Write the partial sum computed by this thread block to global memory
	if(threadIdx.x == 0){
			  lock(mutex);
			  C[0] += sum_per_thread[0];
			  unlock(mutex);
	}
}


__global__ void vector_dot_product_kernel_v2_dp(float *A, float *B, float *C, unsigned int num_elements, int *mutex)
{
	__shared__ float sum_per_thread[THREAD_BLOCK_SIZE];
	unsigned int thread_id = blockIdx.x * blockDim.x + threadIdx.x; // Obtain the index of the thread
	unsigned int stride = blockDim.x * gridDim.x; 
	double sum = 0.0f; 
	unsigned int i = thread_id; 

	while(i < num_elements){
			  sum += (double)A[i] * (double)B[i];
			  i += stride;
	}

	sum_per_thread[threadIdx.x] = (float)sum; // Copy sum to shared memory
	__syncthreads();

	// Reduce the values generated by the thread block to a single value to be sent back to the CPU
	// The following code assumes that the number of threads per block is power of two
	i = blockDim.x/2;
	while(i != 0){
			  if(threadIdx.x < i) 
						 sum_per_thread[threadIdx.x] += sum_per_thread[threadIdx.x + i];
			  __syncthreads();
			  i /= 2;
	}

	// Write the partial sum computed by this thread block to global memory
	if(threadIdx.x == 0){
			  lock(mutex);
			  C[0] += sum_per_thread[0];
			  unlock(mutex);
	}
}


#endif // #ifndef _VECTOR_DOT_PRODUCT_KERNEL_H
