\documentclass[12.0pt]{article}
\usepackage{graphics, graphicx, cite, fancybox, setspace}
\usepackage{amsfonts, amssymb, amsmath, latexsym, epic, eepic, url}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[letterpaper, left=1in, right=1in, top=1in, bottom=1in]{geometry}

\begin{document}

\title{ECEC-413: Introduction to Parallel Computer Architecture \\
Final Exam}
\author{Prof. Naga Kandasamy, ECE Department, Drexel University}
\maketitle %
\date{}

\noindent The exam is due on March 21, 2010. You may work on the problems in teams of up to two people.
\vspace{12pt}

\noindent 1. \textbf{Matrix-Vector Multiplication.} In this problem, you will multiply a dense $n \times n $ matrix $A$ with an $n \times 1$ vector $x$ to yield the $n \times 1$ result vector $y$. The serial algorithm is shown below. \vspace{12pt}

\begin{algorithm}[!h]
\begin{algorithmic}[1]
	\STATE \textbf{procedure} VEC\_MAT\_MULT($A$, $x$, $y$)
    \STATE int $i$, $j$;
	\FOR{$i$ := 0 to $n-1$}
        \STATE $y[i] := 0$;
		\FOR{$j$ := 0 to $n-1$}
				\STATE $y[i] := y[i] + A[i, j] \times x[j]$;
			\ENDFOR
	\ENDFOR
\end{algorithmic}
\end{algorithm}

\noindent Edit the \texttt{vec\_mat\_mult\_on\_device()} function in \texttt{vec\_mat\_mult.cu} and the \texttt{vec\_mat\_mult\_kernel()} function in \texttt{vec\_mat\_mult\_kernel.cu} to complete the functionality of the vector-matrix multiplication on the GPU. Do not change the source code elsewhere (except for adding timing-related code). The size of the matrix is guaranteed to be $512 \times 512$ and the size of the vector will be $512 \times 1$. The CUDA source files for this question are available in a zip file called \texttt{problem\_1.zip}.

Your program should accept no arguments. The application will create a randomly initialized matrix and a vector to multiply. After the GPU-based multiplication kernel is invoked, it will then compute the correct solution using the CPU and compare that solution with the GPU-computed solution. If the solutions match within a certain tolerance, the application will print out ``Test PASSED'' to the screen before exiting. \vspace{12pt}

\noindent E-mail me all of the files needed to run your code as a single zip file called \texttt{solution\_1.zip}. \vspace{12pt}

\noindent This question will be graded on the following parameters:
\begin{itemize}
\item \emph{Correctness}: 50 points. At the very least, use GPU global memory to get your code working.

\item \emph{Performance}: 25 points. This includes the efficient, and in my opinion, clever use of shared memory, appropriately sizing thread granularity, loop unrolling, etc.

\item \emph{Report}: 15 points. A two/three page report describing how you designed your kernel (use code or pseudocode to clarify the discussion) and the amount of speedup obtained over the serial version.

\item \emph{Theory}: 10 points. The GTX 270 GPU can achieve a peak processing rate of about 933 GFLOPs. The memory bandwidth on the device is 141.7 Gb/s. How many floating-point operations must be performed per load operation to achieve the peak processing rate? What is the performance of your kernel, in terms of GFLOPs?
\end{itemize}
\pagebreak

\noindent 2. \textbf{Data Parallel Reduction.} In this problem, you will build on your lab assignment and develop a GPU-based program to sum-reduce a large array of floating-point numbers to a single value. \vspace{12pt}

\noindent Edit the source files \texttt{vector\_reduction.cu} and \texttt{vector\_reduction\_kernel.cu} to complete the functionality of the parallel addition reduction on the GPU.  The size of the array is guaranteed to be equal to $5,000,000$ (five million) elements for this assignment. Note that you may need to define a 2D grid of thread blocks to process an array this large. \vspace{12pt}

\noindent Your program should accept no arguments. The application will create a randomly initialized array to
    process. After the GPU kernel is invoked, it will compute the correct solution value using the CPU, and compare that solution with the GPU-computed solution.  If the solutions match (within a certain tolerance), it will print out ``Test PASSED'' to the screen before exiting. \vspace{12pt}

\noindent You must e-mail me all of the files needed to run your code as a zip file called \texttt{solution\_2.zip}. \vspace{12pt}

\noindent This question will be graded on the following parameters:
\begin{itemize}
\item \emph{Correctness}: 50 points. At the very least, use GPU global memory to get your code working.

\item \emph{Performance}: 25 points. This includes the efficient and clever use of shared memory. Also, for best performance, multiple invocations of the kernel will be necessary. For example, during the first step, each thread block will reduce the set of values assigned to it to a single value (using shared memory) and store these partial sums in GPU global memory. The kernel is invoked again to reduce these partial sums further, and so on.

\item \emph{Report}: 25 points. A two/three page report describing how you designed your kernel (use code or pseudocode to clarify the discussion) and the amount of speedup obtained over the serial version.
\end{itemize}
\pagebreak

\noindent 3. \textbf{Parallelization of a Simple Gaussian Elimination Algorithm.} Consider the problem of solving a system of linear equations of the form
\begin{table}[h]
\centering
\begin{tabular}{ccccc}
$a_{0,0}x_0$ & + $a_{0,1}x_1$ & + $\cdots$ & + $a_{0,n-1}x_{n-1}$ & = $b_0$, \\
$a_{1,0}x_0$ & + $a_{1,1}x_1$ & + $\cdots$ & + $a_{1,n-1}x_{n-1}$ & = $b_1$, \\
. & . & & . & . \\
. & . & & . & . \\
$a_{n-1,0}x_0$ & + $a_{n-1,1}x_1$ & + $\cdots$ & + $a_{n-1,n-1}x_{n-1}$ & = $b_{n-1}$. \\
\end{tabular}
\end{table}

In matrix notation, the above system is written as $Ax = b$ where $A$ is a dense $n \times n$ matrix of coefficients such that $A[i, j] = a_{i, j}$, $b$ is an $n \times 1$ vector $[b_0, b_1, \ldots, b_{n-1}]^T$, and $x$ is the desired solution vector $[x_0, x_1, \ldots, x_{n-1}]^T$. From here on, we will denote the matrix elements $a_{i, j}$ and $x_i$ by $A[i, j]$ and $x[i]$, respectively. A system of equations $Ax = b$ is usually solved in two stages. First, through a set of algebraic manipulations, the original system of equations is reduced to an upper triangular system of the form
\begin{table}[h]
\centering
\begin{tabular}{cccccc}
$x_0$ & + $u_{0,1}x_1$ & + $u_{0, 2}x_2$ & + $\cdots$ & + $u_{0,n-1}x_{n-1}$ & = $y_0$, \\
& $x_1$ & + $u_{1, 2}x_2$ & + $\cdots$ & + $u_{1,n-1}x_{n-1}$ & = $y_1$, \\
& & & & . & . \\
& & & & . & . \\
& & & & $x_{n-1}$ & = $y_{n-1}$. \\
\end{tabular}
\end{table}

\noindent We write the above system as $Ux = y$, where $U$ is a unit upper-triangular matrix, that is, one where the subdiagonal entries are zero and all principal diagonal entries are equal to one. More formally, $U[i, j] = 0$ if $i > j$, otherwise $U[i, j] = u_{i, j}$, and furthermore, $U[i, i] = 1$ for $0 \le i < n$. In the second stage of solving a system of linear equations, the upper-triangular system is solved for the variables in reverse order, from $x[n-1]$ to $x[0]$ using a procedure called back-substitution. \vspace{6pt}

\noindent A serial implementation of a simple Gaussian elimination algorithm is shown below.

\begin{algorithm}[!h]
\begin{algorithmic}[1]
	\STATE \textbf{procedure} GAUSS\_ELIMINATE($A$, $b$, $y$)
    \STATE int $i$, $j$, $k$;
    \FOR{$k$ := 0 to $n-1$}
        \FOR{$j$ := $k+1$ to $n-1$}
            \STATE $A[k, j]$ := $A[k, j]/A[k, k]$; $\quad$ /* Division step. */
        \ENDFOR
        \STATE $y[k]$ := $b[k]/A[k, k]$;
        \STATE $A[k, k] := 1$;
        \FOR{$i$ := $k+1$ to $n - 1$}
            \FOR{$j$ := $k+1$ to $n - 1$}
                \STATE $A[i, j]$ := $A[i, j]$ - $A[i, k] \times A[k, j]$; $\quad$ /* Elimination step. */
            \ENDFOR
            \STATE $b[i]$ := $b[i] - A[i, k] \times y[k]$;
            \STATE $A[i, k] := 0$;
        \ENDFOR
    \ENDFOR
\end{algorithmic}
\end{algorithm}

\noindent The algorithm converts the system of linear equations $Ax = b$ into a unit upper-triangular system $Ux = y$. We assume that the matrix $u$ shares storage with $A$ and overwrites the upper-triangular portion of $A$. So, the element $A[k, j]$ computed in line 5 of the code is actually $U[k, j]$. Similarly, the element $A[k, k]$ that is equated to 1 in line 8 is $U[k, k]$. Also, our program assumes that $A[k, k] \neq 0$ when it is used as a divisor in lines 5 and 7. So, our implementation is numerically unstable, though it should not be a concern for this assignment.

For $k$ ranging from 0 to $n - 1$, the Gaussian elimination procedure systematically eliminates the variable $x[k]$ from equations $k+1$ to $n-1$ so that the matrix of coefficients becomes upper-triangular. In the $k^{\texttt{th}}$ iteration of the outer loop (line 3), an appropriate multiple of the $k^{\texttt{th}}$ equation is subtracted from each of the equations $k+1$ to $n-1$

This problem asks you to develop a parallel formulation of \texttt{GAUSS\_ELIMINATE} for the GPU.  \noindent Edit the \texttt{gauss\_eliminate\_on\_device()} function in \texttt{gauss\_eliminate.cu} and the \texttt{gauss\_eliminate\_\_kernel()} function in \texttt{gauss\_eliminate\_kernel.cu} to complete the functionality of Gaussian elimination on the GPU. Do not change the source code elsewhere (except for adding timing-related code). The size of the matrix $A$ is guaranteed to be $2048 \times 2048$. The CUDA source files for this question are available in a zip file called \texttt{problem\_2.zip}.

Your program should accept no arguments. After the GPU-based kernel computes the corresponding upper-diagonal matrix $u$, the CPU will compute the correct solution and compare the results. If the solutions match within a certain tolerance, the application will print out ``Test PASSED'' to the screen before exiting. Please use the matrices that I have provided for your tests. \vspace{12pt}

\noindent E-mail me all of the files needed to run your code as a single zip file called \texttt{solution\_2.zip}. \vspace{12pt}

\noindent This question will be graded on the following parameters:
\begin{itemize}
\item \emph{Correctness}: 50 points. At the very least, you must use GPU global memory to get your code working.

\item \emph{Performance}: 25 points, that includes the efficient, and in my opinion, clever use of shared memory, appropriately sizing the thread granularity, etc.

\item \emph{Report}: 25 points. A two/three page report describing how you designed your kernel (use code or pseudocode if that helps the discussion) and the amount of speedup obtained over the serial version.
\end{itemize}

\end{document}
